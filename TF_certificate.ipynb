{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-certificate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOvaxk4cAE2CSaLg9GGW/mD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soerenml/tensorflow-certificate/blob/master/TF_certificate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo5lvmWsviqc",
        "colab_type": "text"
      },
      "source": [
        "# Tensorflow certificate\n",
        "\n",
        "The following certificate follows the tutorials and guides from the official tensorflow [website](https://www.tensorflow.org/tutorials) and Geron (2019). In most cases, the code is taken from the examples with argumentation and explainations done by me.\n",
        "\n",
        "The notebook is intented as a general introdution to TF as well as preperation for the certificate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_IqYfImU3Iu",
        "colab_type": "text"
      },
      "source": [
        "# 0) Load packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXJTk_zRxg2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install relevant packages first\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Cleaner model building\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras import losses\n",
        "\n",
        "# Text classification\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "import string\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "print(\"Tensorflow Version: {version}\".format(version=tf.__version__))\n",
        "\n",
        "# Make the program deterministic\n",
        "# https://www.tensorflow.org/api_docs/python/tf/random/set_seed\n",
        "tf.random.set_seed(11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trXKeyISPvYU",
        "colab_type": "text"
      },
      "source": [
        "**Check the hardware**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m2AQBkgPRMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Is there a GPU available: \"),\n",
        "print(tf.config.experimental.list_physical_devices(\"GPU\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6YM_ZltKebj",
        "colab_type": "text"
      },
      "source": [
        "# 1) Getting familiar with Tensors\n",
        "\n",
        "Differnt kinds of tensors exist: <code>tf.Variable, tf.constant, tf.placeholder, tf.SparseTensor, and tf.RaggedTensor</code>.\n",
        "\n",
        "Tensors differ to numpy arrays in a way that:\n",
        "+ they can be loaded into accelerators (GPU/TPU)\n",
        "+ are not immutable\n",
        "\n",
        "The transition between numpy arrays and tensors is fluid. \n",
        "Nevertheless, while numpy arrays live in memory, tensors can live in GPU/TPU which causes incompatability ([Source](https://www.tensorflow.org/tutorials/customization/basics))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwkpYBOxKjv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's create a two dimensional Tensor\n",
        "x = tf.constant(\n",
        "    [\n",
        "     [1,2,3], [4,5,6]\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Tensors attributes\n",
        "print(x)\n",
        "print(x.shape)\n",
        "print(x.dtype)\n",
        "\n",
        "# Tensors can also be indexed (same as numpy arrays)\n",
        "print(x[1,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I5J6a1JEBaL",
        "colab_type": "text"
      },
      "source": [
        "**Conversion and performance**\n",
        "\n",
        "TF does not allow automatic conversion as this can lead to performance issues and unexpected incompatability issues.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5SfKTVsEBgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zf = tf.constant(18, dtype=tf.float64)\n",
        "zi = tf.constant(18, dtype=tf.int64)\n",
        "print(zf, zi)\n",
        "\n",
        "# When adding the two, an error get thrown.\n",
        "zf + zi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAbfoFS-FFpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To fix this, we need to find a mutual format using cast\n",
        "zf + tf.cast(zi, dtype=tf.float64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwlEsGOzFUvf",
        "colab_type": "text"
      },
      "source": [
        "**The fluid relationship between tensors and arrays**\n",
        "\n",
        "Numpy can be used for Tensor operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBh1GoC2Cypo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = np.array([1,2,3,4,5])\n",
        "print(tf.square(n))\n",
        "print(np.square(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix6yNeKRFx0C",
        "colab_type": "text"
      },
      "source": [
        "**Variables**\n",
        "\n",
        "All Tensor data types are immutable - except variables. For an overview of differnt TensorTypes see Geron 2019 p. 383.\n",
        "\n",
        "In particular during training, immutable datatypes are needed (e.g. updating weights). For such cases variables are used which can be updates with <code>assign</code>. Same as for other datatypes, there is no automatic conversion and datatypes must be the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkpFy4JnFx8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_var = tf.Variable(10, dtype=tf.int64)\n",
        "print(t_var)\n",
        "\n",
        "# Update the variable\n",
        "t_var.assign(11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Toj2wYzPLm1W",
        "colab_type": "text"
      },
      "source": [
        "**TF Functions**\n",
        "\n",
        "TF Functions cab be used to add Python code to a TF-Graph. You should have in mind that only TF constructs can be used. Hence, the operations must be convertible to TF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRwYWLDKKJ0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def tf_power(x):\n",
        "  return x**11\n",
        "\n",
        "# Run the tensorflow function\n",
        "print(tf_power(7))\n",
        "\n",
        "# Or simply run it as a Python function\n",
        "print(tf_power.python_function(7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-QPG38qMeaN",
        "colab_type": "text"
      },
      "source": [
        "# 2) Preprocessing data\n",
        "\n",
        "Data preprocessing is pivotal and one of the most complicated parts of Tensorflow. There are two core components: tf.transform and tf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n9uJpNXMcuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = tf.range(10)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "print(dataset)\n",
        "\n",
        "# Iterate over the dataset\n",
        "for item in dataset:\n",
        "  print(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUMFArnrKzqd",
        "colab_type": "text"
      },
      "source": [
        "**Shuffle the dataset**\n",
        "\n",
        "Geron (2019) uses the analogy of shuffling a card deck (cards are sorted with the highest cards first): when shuffling you would like to shuffle the whole card deck and pick a card. What you not want to do is just pick the first three cards and shuffle them, pick one, add the next card (number four) and repeat. The reason is obvious: if the whole deck would be sorted your 'shuffled deck' have descending card value (higher cards first, lower cards at the end).\n",
        "\n",
        "Long story short: set buffer size = number of observations in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1S3iSSmDnH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = tf.range(10)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "\n",
        "# When added functions to map to not use ()\n",
        "def pre(x): \n",
        "  return x*2\n",
        "\n",
        "# Let's add additional transformation\n",
        "dataset = (dataset\n",
        "           .map(pre)\n",
        "           .shuffle(buffer_size=30) # Shuffle the dataset\n",
        "           .repeat(3) # repeat the dataset three times\n",
        "           .batch(7) # take seven observations from each dataset\n",
        "           )\n",
        "\n",
        "# Important: we repeat the dataset three times (i.e. 30 items)\n",
        "# This is why we have 4*7 and 1*2 as batch size.\n",
        "for item in dataset:\n",
        "  print(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOUlbZGwo34-",
        "colab_type": "text"
      },
      "source": [
        "**First, let's get some data some github**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WauCvqPso4EL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sets = [\"train.csv\", \"test.csv\"]\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/agconti/kaggle-titanic/master/data/train.csv\"\n",
        "df_train = pd.read_csv(url, error_bad_lines=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7GkOhm4pN_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reorder the columns\n",
        "cols = df_train.columns.tolist()\n",
        "cols = cols[2:] + cols[0:1] + cols[1:2]\n",
        "df_train = df_train[cols]\n",
        "df_train = df_train.drop(columns=['Name'])\n",
        "df_train.to_csv('./train.csv', index=False)\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21vYNErEJL_j",
        "colab_type": "text"
      },
      "source": [
        "**Working high speed input pipeline**\n",
        "\n",
        "A lot of different things are happening here at the same time:\n",
        "\n",
        "+ <code>prefetch(1)</code> - we keep one batch always at hand. Hence, while one batch has already been fed to the training cyle, another has been already loaded.\n",
        "+ <code>num_of_parallel_calls</code> - helps to split up preprocessing horizontally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-jSR9ybJRgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath=\"/content/train.csv\"\n",
        "\n",
        "n_inputs = 10\n",
        "def preprocess(line):\n",
        "  \"\"\"\n",
        "  Preprocessing function for the csv file.\n",
        "\n",
        "  Parameters: \n",
        "    line: row of a csv file\n",
        "  Return:\n",
        "    x: features\n",
        "    y: target\n",
        "  \"\"\"\n",
        "  defs = [0.] * n_inputs + [tf.constant([], dtype=tf.int32)] # define default values\n",
        "  fields = tf.io.decode_csv(records=line, record_defaults=defs) # read csv file\n",
        "  x = tf.stack(fields[:-1])\n",
        "  y = tf.stack(fields[-1:])\n",
        "  return x, y\n",
        "\n",
        "def csv_reader_dataset(filepath):\n",
        "  \"\"\"\n",
        "  Create tf.dateset\n",
        "  \n",
        "  Parameters:\n",
        "    filpath: path of the file to read\n",
        "  Return:\n",
        "    tensorflow dataset object\n",
        "  \"\"\"\n",
        "  dataset=tf.data.Dataset.list_files(filepath)\n",
        "  dataset=dataset.shuffle(buffer_size=30) # Shuffle the dataset \n",
        "  dataset=dataset.map(preprocess,\n",
        "                      num_parallel_calls=tf.data.experimental.AUTOTUNE) # Run preprocessing\n",
        "  dataset=dataset.repeat(1) # Repeat the dataset three times\n",
        "  return dataset.batch(20).prefetch(1) # Select batch and prefetch\n",
        "\n",
        "train_set = csv_reader_dataset(filepath)\n",
        "print(train_set)\n",
        "\n",
        "\n",
        "for line in train_set.take(1):\n",
        "  print(line.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUfRGGGhVA87",
        "colab_type": "text"
      },
      "source": [
        "# 3) Working with structured data (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpFNFw-zVBEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load dataset\n",
        "sets = [\"train.csv\", \"test.csv\"]\n",
        "for i in sets:\n",
        "  url = \"https://raw.githubusercontent.com/agconti/kaggle-titanic/master/data/{dataset}\".format(dataset = i)\n",
        "  pd.read_csv(url, error_bad_lines=False).to_csv(\"./{dataset}\".format(dataset = i))\n",
        "\n",
        "filepath=\"/content/train.csv\"\n",
        "\n",
        "# Define preprocessing function\n",
        "n_inputs = 10\n",
        "def preprocess(line):\n",
        "  \"\"\"\n",
        "  There is a lot of back and forth with arrays and lists.\n",
        "  I am using this approach to select items in the list.\n",
        "  \"\"\"\n",
        "  defs = [0.] * n_inputs + [tf.constant([])] # define default values\n",
        "  fields = np.array(tf.io.decode_csv(records=line, record_defaults=defs)) # read csv file\n",
        "  x = tf.stack(fields[[0,2,3,4,5,6,7,8,9,10]].tolist()) \n",
        "  y = tf.stack(fields[[1]].tolist())\n",
        "  return x, y\n",
        "\n",
        "def csv_reader_dataset(filepath):\n",
        "  \"\"\"\n",
        "  Create tf.dateset\n",
        "  \"\"\"\n",
        "  dataset=tf.data.Dataset.list_files(filepath)\n",
        "  dataset=dataset.shuffle(buffer_size=30) # Shuffle the dataset \n",
        "  dataset=dataset.map(preprocess,\n",
        "                      num_parallel_calls=tf.data.experimental.AUTOTUNE) # Run preprocessing\n",
        "  dataset=dataset.repeat(3) # Repeat the dataset three times\n",
        "  return dataset.batch(7).prefetch(1) # Select batch and prefetch\n",
        "\n",
        "train_set = csv_reader_dataset(filepath)\n",
        "train_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCIT81Rfwt0y",
        "colab_type": "text"
      },
      "source": [
        "# 4) Image recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSvCljH74oWI",
        "colab_type": "text"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX-va44Xx45c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPj9oEyj1UYR",
        "colab_type": "text"
      },
      "source": [
        "## Getting familiar with data shapes\n",
        "\n",
        "Most of ML is linear algebra. Hence, it is important to become familiar with vectors, matrices, tensors and multidimensional spaces in general. Our training data essentially uses two shapes: 3-dimensional tensors for training and validation containing our features and one dimensional tensors (vectors) for validation containing our targets.\n",
        "\n",
        "In our case we have a 60000x28x28 feature tensor. You can imagine 60k as the numbers of images and 28x28 as the image grid (x, y positions of the pixels). You can imagine the target tensor as a vector with a lenght of 60000.\n",
        "\n",
        "P.S.: Because we have a three-dimensional tensor our images are black and white. E.g. we lack a fourth dimension representing colours (RGB).\n",
        "\n",
        "**Source for the plots can be found [here](https://matplotlib.org/3.1.0/gallery/pyplots/fig_axes_labels_simple.html).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CizZSOnOST4h",
        "colab_type": "text"
      },
      "source": [
        "### Formal description of datashapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pclb-9lIO8u0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is an example of how a three dimensional vector is structure.\n",
        "# You can imagine it as several matrices.\n",
        "\n",
        "x = np.array(\n",
        "    [\n",
        "     [\n",
        "      [1,2,3],\n",
        "      [4,5,6],\n",
        "      [7,8,9]\n",
        "     ],\n",
        "     [\n",
        "      [1,2,3],\n",
        "      [4,5,6],\n",
        "      [7,8,9]\n",
        "     ]\n",
        "    ]\n",
        "    )\n",
        "\n",
        "print(x)\n",
        "\n",
        "# The shape indicates two matrices with a 3x3 dimension.\n",
        "print(x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWK-TVWfSY8d",
        "colab_type": "text"
      },
      "source": [
        "### A visual description of datashapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iM-V2UP1ax_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D \n",
        "# Change styles of graphs\n",
        "import matplotlib as mpl\n",
        "mpl.style.use('seaborn') \n",
        "\n",
        "print(\"Properties of the traning vector (train_x): {}\".format(x_train.shape))\n",
        "\n",
        "x, y, z = np.indices((28, 100, 28))\n",
        "cube1 = (x < 28) & (y < 1) & (z < 28)\n",
        "voxels = cube1 \n",
        "colors = np.empty(voxels.shape, dtype=object)\n",
        "colors[cube1] = 'blue'\n",
        "fig = plt.figure()\n",
        "ax = fig.gca(projection='3d')\n",
        "ax.voxels(voxels, facecolors=colors, edgecolor='k')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZHhHoYFLUDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y, z = np.indices((28, 100, 28))\n",
        "cube1 = (x < 1) & (y < 1) & (z < 100)\n",
        "voxels = cube1 \n",
        "colors = np.empty(voxels.shape, dtype=object)\n",
        "colors[cube1] = 'blue'\n",
        "fig = plt.figure()\n",
        "ax = fig.gca(projection='3d')\n",
        "ax.voxels(voxels, facecolors=colors, edgecolor='k')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKjL2SRgM00V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(x_train[0,:,:], cmap='gray')\n",
        "plt.show()\n",
        "print(\"Number: {}\".format(y_train[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vreHnCscOcDu",
        "colab_type": "text"
      },
      "source": [
        "## Creating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "799CfKR4UxhK",
        "colab_type": "text"
      },
      "source": [
        "**Excurse numerical computation**\n",
        "\n",
        "We are not using a softmax function for the last layer in our model. This is due to the fact that a softmax function is not necessary numerically stable. A compunter needs to stop calculating numbers at a certain point in time. This might lead to numbers extremely small (underflow) or large (overflow) values resulting in 0 or $\\infty$ leading to complications with softmax:\n",
        "\n",
        "$x_{i} = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$\n",
        "\n",
        "\n",
        "Btw. if you ever asked yourself, 'why are we using the log-likelihood'... for the the same reason. The log helps us to avoid underfitting.\n",
        "\n",
        "[Source](http://www.deeplearningbook.org/contents/numerical.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-NyCdMQUfK5",
        "colab_type": "text"
      },
      "source": [
        "**Layer structure**\n",
        "\n",
        "Our first layer has an output shape of (None, 784). We use <code>None</code> for all layers, as we did not define the numbers of examples per shape - which is not necessary overall. \n",
        "\n",
        "Our first layer is a flattening layer transposing a n-dimensional- matrix into a one-dimensional vector (28x28=784).\n",
        "\n",
        "Important, the layer has no weights, as it's the input layer.\n",
        "\n",
        "The following layers are dense- or fully connected layers. Both layers have weights which are the product of shapes (e.g. (784+1)x10, (10+1)x10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffg7fmY8x59k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build model\n",
        "model = Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28)),\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Show model structure\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3uu_uAbTusV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the model as .png\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model,\n",
        "           to_file='model_plot.png',\n",
        "           show_shapes=True,\n",
        "           show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMUVC_3lymtD",
        "colab_type": "text"
      },
      "source": [
        "The layers are [initialized](https://keras.io/api/layers/initializers/). Hence, from a functional perspective, the model can be already used for prediciont.\n",
        "\n",
        "Neverthless, the model is not trained yet. Let's see, how the untrained model performs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElWs4ZMFyADc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ingest image\n",
        "predictions_untrained = model(x_train[:1]).numpy()\n",
        "\n",
        "# Use a softmax function to increase interpretability.\n",
        "# Softmax functions return 'regular' probabilities (SUM(p(i)) = 1)\n",
        "predictions_untrained = tf.nn.softmax(predictions_untrained).numpy()\n",
        "print(predictions_untrained)\n",
        "\n",
        "print(\"\"\"\n",
        "Predicted value should be: {label}\\n\n",
        "Total probabilities sum up to: {sums}\"\"\".format(\n",
        "    label=y_train[0],\n",
        "    sums=np.sum(predictions_untrained[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYMY9tpAk9yB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot probabilities\n",
        "def plot_bar(input):\n",
        "  '''Plotting function for barcharts'''\n",
        "  objects = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "  y_pos = np.arange(len(objects))\n",
        "  plt.bar(y_pos, input, align='center', alpha=0.5)\n",
        "  plt.xticks(y_pos, objects)\n",
        "  plt.ylabel('Likelihood')\n",
        "  plt.xlabel('Number')\n",
        "  plt.show()\n",
        "\n",
        "plot_bar(predictions_untrained[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WlJb_q4hEaN",
        "colab_type": "text"
      },
      "source": [
        "### Use tf.Data for faster ingestion\n",
        "\n",
        "the main reason to use tf.data is efficiency. For further information see [here](https://www.tensorflow.org/guide/data_performance)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc5o39CHhQ_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_ds = train_ds.shuffle(10000).batch(32)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4mdSNAaJ45s",
        "colab_type": "text"
      },
      "source": [
        "### Define callbacks\n",
        "\n",
        "Callbacks are function which are executed after each epoch. Callbacks can be used to calculate custom matrics or -even more important- to interrupt training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIVmbfLlh6R_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tensorboard callback\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
        "\n",
        "# Early stopping callback\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.9):\n",
        "      print(\"\\n Stopped training: \" +str(logs.get('accuracy')))\n",
        "      self.model.stop_training = True\n",
        "\n",
        "# Important, the class needs to be initiated: callbacks = myCallback()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJTH1Si8J_bn",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5p0lQOo1ywO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "callback_stop = myCallback()\n",
        "\n",
        "hist = model.fit(train_ds,\n",
        "                 validation_data=test_ds, \n",
        "                 epochs=100, # setting the epochs up for the early stopping to work.\n",
        "                 callbacks=[tensorboard_callback, callback_stop])\n",
        "\n",
        "# Let's have a look into our hist object\n",
        "print(hist.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZrIFz7u49D6",
        "colab_type": "text"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwOgIf97iNgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir \"./logs\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_L6srG05AhE",
        "colab_type": "text"
      },
      "source": [
        "## Making predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHb1JhWSZ85b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make pred. using model.predict\n",
        "example_result = model.predict((x_train[:1]))\n",
        "\n",
        "# In order to get meanigful probabilities we can interpret\n",
        "# we are using a softmax function.\n",
        "predictions_trained = tf.nn.softmax(example_result)\n",
        "plot_bar(predictions_trained[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59xc4_9m2IiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's compare model performance between test and training\n",
        "loss, accuracy = model.evaluate(x_test,  y_test, verbose=2)\n",
        "\n",
        "print(\"\"\"\n",
        "Model performance training: {eval}\\n\n",
        "Model performance test: {test}\n",
        "\"\"\".format(eval=accuracy,\n",
        "           test=hist.history['accuracy'][4]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM4hfyYeXM-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "rm -r ./logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEK9EeQfqbzI",
        "colab_type": "text"
      },
      "source": [
        "## Using CNNs\n",
        "\n",
        "Let's use CNNs for the same task. CNN are characterized by two key components:\n",
        "\n",
        "- Convolutional-layers\n",
        "- Pooling-layers\n",
        "\n",
        "Convolutional layers are \"filter\" layers helping to emphasize \"features in an image\" (see [Kernel](https://en.wikipedia.org/wiki/Kernel_(image_processing)) for further information)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcMQ5zcTu70W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# TODO - add callback class mycallback(tf.keras.call)\n",
        "\n",
        "\n",
        "def reshape_norm(train, test):\n",
        "  \"\"\"\n",
        "  Reshape and normalize data\n",
        "  \"\"\"\n",
        "  training_img=train.reshape(60000, 28, 28, 1)\n",
        "  training_img=training_img / 255.0\n",
        "  test_img = test.reshape(10000, 28, 28, 1)\n",
        "  test_img=test_img/255.0\n",
        "  return training_img, test_img\n",
        "\n",
        "train_img, test_img = reshape_norm(training_images, test_images)\n",
        "\n",
        "# Build model\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(1000, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(train_img, training_labels, epochs=5)\n",
        "\n",
        "test_loss = model.evaluate(test_img, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfOCJEP8wVIr",
        "colab_type": "text"
      },
      "source": [
        "### How convolutional layers are working\n",
        "\n",
        "TODO - write out math behind.\n",
        "See noteboook here: https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%206%20-%20Lesson%203%20-%20Notebook.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w5ay-0aW2DB",
        "colab_type": "text"
      },
      "source": [
        "# ImageGenerator\n",
        "\n",
        "The image generator is very useful as it automatically labels images according to the folder structure. It is not part of tf.data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKIIbe67EzFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n",
        "    -O /tmp/horse-or-human.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNsM1MFgwR7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/horse-or-human')\n",
        "zip_ref.close()\n",
        "\n",
        "# Directory with our training horse pictures\n",
        "train_horse_dir = os.path.join('/tmp/horse-or-human/horses')\n",
        "\n",
        "# Directory with our training human pictures\n",
        "train_human_dir = os.path.join('/tmp/horse-or-human/humans')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANQSD7e0EPWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR8qn71uHiMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb4y-KBLHzIs",
        "colab_type": "text"
      },
      "source": [
        "## Preprocesing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmIp0Bs6HzQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = ImageDataGenerator(rescale=1/255).flow_from_directory(\n",
        "        '/tmp/horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(300, 300),  # All images will be resized to 300x300\n",
        "        batch_size=128,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xy0FhB2IZSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=8,  \n",
        "    epochs=15,\n",
        "    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF9Q-DL0LAom",
        "colab_type": "text"
      },
      "source": [
        "### Use the model to predict (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZI-98gDLAy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "\n",
        "img = image.load_img(path, target_size=(300, 300))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "images = np.vstack([x])\n",
        "outcome = model.predict(images, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REFG4vKyLnzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "# Let's define a new Model that will take an image as input, and will output\n",
        "# intermediate representations for all layers in the previous model after\n",
        "# the first.\n",
        "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
        "#visualization_model = Model(img_input, successive_outputs)\n",
        "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
        "# Let's prepare a random input image from the training set.\n",
        "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
        "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
        "img_path = random.choice(horse_img_files + human_img_files)\n",
        "\n",
        "img = load_img(img_path, target_size=(300, 300))  # this is a PIL image\n",
        "x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\n",
        "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n",
        "\n",
        "# Rescale by 1/255\n",
        "x /= 255\n",
        "\n",
        "# Let's run our image through our network, thus obtaining all\n",
        "# intermediate representations for this image.\n",
        "successive_feature_maps = visualization_model.predict(x)\n",
        "\n",
        "# These are the names of the layers, so can have them as part of our plot\n",
        "layer_names = [layer.name for layer in model.layers[1:]]\n",
        "\n",
        "# Now let's display our representations\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "  if len(feature_map.shape) == 4:\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
        "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
        "    # The feature map has shape (1, size, size, n_features)\n",
        "    size = feature_map.shape[1]\n",
        "    # We will tile our images in this matrix\n",
        "    display_grid = np.zeros((size, size * n_features))\n",
        "    for i in range(n_features):\n",
        "      # Postprocess the feature to make it visually palatable\n",
        "      x = feature_map[0, :, :, i]\n",
        "      x -= x.mean()\n",
        "      x /= x.std()\n",
        "      x *= 64\n",
        "      x += 128\n",
        "      x = np.clip(x, 0, 255).astype('uint8')\n",
        "      # We'll tile each filter into this big horizontal grid\n",
        "      display_grid[:, i * size : (i + 1) * size] = x\n",
        "    # Display the grid\n",
        "    scale = 20. / n_features\n",
        "    plt.figure(figsize=(scale * n_features, scale))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoB4G73ikvz7",
        "colab_type": "text"
      },
      "source": [
        "# 5) Text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REUvXusKqTOj",
        "colab_type": "text"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUijFv9BkzEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url, untar=True, cache_dir='.')\n",
        "print(dataset)\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Flq41t-SqJLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "rm -r datasets/aclImdb/train/pos/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EhUoSxIqwRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO - I do not understand the splitting.\n",
        "\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "# Define training set\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'datasets/aclImdb/train', \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='training', \n",
        "    seed=seed)\n",
        "\n",
        "# Define validation set\n",
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'datasets/aclImdb/train', \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='validation', \n",
        "    seed=seed)\n",
        "\n",
        "# Define test set\n",
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'datasets/aclImdb/train', \n",
        "    batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQkrBj8QryKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Have a look at the data\n",
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(1):\n",
        "    print(\"Review\", text_batch.numpy()[i])\n",
        "    print(\"Label\", label_batch.numpy()[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ectRqZUKuIDc",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess the data\n",
        "\n",
        "- Standardize: clean the data (e.g. punctuation, HTML)\n",
        "- Tokenize (split sentences into words)\n",
        "- Vectorize (create embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDjKDaQlmR0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standardize / clean the data\n",
        "# TODO - why is my alternative version not working?\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(\n",
        "      stripped_html,\n",
        "      '[%s]' % re.escape(string.punctuation),\n",
        "      '')\n",
        "\n",
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features, # number of words\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpffpTlGmc5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO understand this function.\n",
        "# Make a text-only dataset (without labels), then call adapt\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npR2TbbxmfCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBHSgEiQ7jsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YaWZm-LBPVW",
        "colab_type": "text"
      },
      "source": [
        "## Create the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5chaDGG1BPai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "embedding_dim = 16\n",
        "\n",
        "model = Sequential([\n",
        "  layers.Embedding(max_features + 1, embedding_dim),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvSBttNFUghf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the model\n",
        "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer='adam',\n",
        "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))\n",
        "\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs,\n",
        "    callbacks=tensorboard_callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byFb1fOZVjt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNFBeDL0Xr32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir \"./logs\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS8WdVsyYVBe",
        "colab_type": "text"
      },
      "source": [
        "## Export the model\n",
        "\n",
        "We defining the training pipeline, we had to standardize, tokenize and vectorize. We can bake in those three steps, reusing our vectorization layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymXhWkdlYVK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO - Understand wherer the differnces in loss come from.\n",
        "\n",
        "# Create new model\n",
        "export_model = tf.keras.Sequential([\n",
        "  vectorize_layer,\n",
        "  model,\n",
        "  layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "# Re-compile model as we added additional layers\n",
        "export_model.compile(\n",
        "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
        "print(accuracy)\n",
        "\n",
        "# without 21s 11ms/step - loss: 1.6500 - accuracy: 0.8131\n",
        "# with 21s 11ms/step - loss: 0.3836 - accuracy: 0.8191\n",
        "# included in model: "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}